# -*- coding: utf-8 -*-
"""CE807_2211527.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1juw1cvPeHOf5N6CTPLJzpVakPMg6Er0V
"""

import pandas as pd

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

# Load train, test, and variable datasets
train_data = pd.read_csv('/content/drive/MyDrive/train.csv')
test_data = pd.read_csv('/content/drive/MyDrive/test.csv')
variable_data = pd.read_csv('/content/drive/MyDrive/valid.csv')

# Display train dataset
print("Train Dataset:")
print(train_data.head())

# Display test dataset
print("\nTest Dataset:")
print(test_data.head())

# Display variable dataset
print("\nVariable Description Dataset:")
print(variable_data)

# Import necessary libraries
import numpy as np
import pandas as pd

# Load the train dataset
train_df = pd.read_csv('/content/drive/MyDrive/train.csv')

# Explore the train dataset
print(train_df.head())

# Model Selection
generative_model = 'Naive Bayes'  # Example: Naive Bayes
discriminative_model = 'Logistic Regression'  # Example: Logistic Regression

# Justification
generative_model_justification = """
Naive Bayes is a classic generative model that works well for text classification tasks like this.
It assumes independence among features and calculates the probability of each class given the features.
Additionally, Naive Bayes is computationally efficient and often performs well in practice, making it a suitable choice for this task.
"""

discriminative_model_justification = """
Logistic Regression is a widely-used discriminative model that directly learns the decision boundary between classes.
It works well for binary classification tasks like this and is easy to interpret. Despite its simplicity, Logistic Regression often performs
competitively with more complex models, especially when the feature space is not too large.
"""

# Print model selection and justification
print(f"Generative Model: {generative_model}")
print("Justification:")
print(generative_model_justification)

print(f"\nDiscriminative Model: {discriminative_model}")
print("Justification:")
print(discriminative_model_justification)

# Import necessary libraries
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Load the train dataset
train_df = pd.read_csv('/content/drive/MyDrive/train.csv')

# Preprocessing and feature extraction
vectorizer = TfidfVectorizer(max_features=10000)  # Using TF-IDF for feature extraction
X = vectorizer.fit_transform(train_df['comment'])
y = train_df['toxicity']

# Split the dataset into train and validation sets
X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Generative model (Naive Bayes)
nb_classifier = MultinomialNB()
nb_classifier.fit(X_train, y_train)

# Train Discriminative model (Logistic Regression)
lr_classifier = LogisticRegression(max_iter=1000)
lr_classifier.fit(X_train, y_train)

# Evaluation
def evaluate_model(classifier, X, y_true):
    y_pred = classifier.predict(X)
    accuracy = accuracy_score(y_true, y_pred)
    report = classification_report(y_true, y_pred)
    return accuracy, report

# Evaluate Generative model
nb_accuracy, nb_report = evaluate_model(nb_classifier, X_valid, y_valid)
print("Generative Model (Naive Bayes) Performance:")
print(f"Accuracy: {nb_accuracy}")
print("Classification Report:")
print(nb_report)

# Evaluate Discriminative model
lr_accuracy, lr_report = evaluate_model(lr_classifier, X_valid, y_valid)
print("\nDiscriminative Model (Logistic Regression) Performance:")
print(f"Accuracy: {lr_accuracy}")
print("Classification Report:")
print(lr_report)

import pandas as pd

# Load the validation dataset
validation_df = pd.read_csv('/content/drive/MyDrive/valid.csv')

# Assuming you have trained and loaded your Naive Bayes (nb_classifier) and Logistic Regression (lr_classifier) models,
# and also loaded your vectorizer (vectorizer)

# Predict toxicity labels using both models
validation_df['predicted_toxicity_nb'] = nb_classifier.predict(vectorizer.transform(validation_df['comment']))
validation_df['predicted_toxicity_lr'] = lr_classifier.predict(vectorizer.transform(validation_df['comment']))

# Select 5 diverse examples
sample_examples = validation_df.sample(n=5, random_state=42)

# Display and analyze the selected examples
for idx, example in sample_examples.iterrows():
    print(f"Example {idx + 1}:")
    print("Comment:", example['comment'])
    print("True Label (Toxicity):", example['toxicity'])
    print("Predicted Label (Naive Bayes):", example['predicted_toxicity_nb'])
    print("Predicted Label (Logistic Regression):", example['predicted_toxicity_lr'])
    print()

# Compare model performance (assuming you have already generated classification reports)
print("Comparing Model Performance:")
print("Naive Bayes Model:")
print(nb_report)
print("Logistic Regression Model:")
print(lr_report)

import pandas as pd

# Replace 'xxx' with the actual values for your dataset
total_train = 1000
percent_class_A_train = 60
percent_class_B_train = 40

total_valid = 500
percent_class_A_valid = 70
percent_class_B_valid = 30

total_test = 700
percent_class_A_test = 55
percent_class_B_test = 45

# Create a DataFrame for dataset details
dataset_details = pd.DataFrame({
    'Dataset': ['Train', 'Valid', 'Test'],
    'Total': [total_train, total_valid, total_test],
    '% Class A': [percent_class_A_train, percent_class_A_valid, percent_class_A_test],
    '% Class B': [percent_class_B_train, percent_class_B_valid, percent_class_B_test]
})

# Print the DataFrame
print("Dataset Details:")
print(dataset_details)

import pandas as pd

# Replace 'xxx' with the actual F1 scores for your models
f1_score_model_1 = 0.85
f1_score_model_2 = 0.87
f1_score_sota = 0.90  # Replace 'xxx' with the actual SoTA F1 score

# Create a DataFrame for model performance
model_performance = pd.DataFrame({
    'Model': ['Model 1', 'Model 2', 'SoTA'],
    'F1 Score': [f1_score_model_1, f1_score_model_2, f1_score_sota]
})

# Print the DataFrame
print("Model Performance:")
print(model_performance)
model_performance.to_csv('test.csv',index=False)
print("\noutput saved in test.csv")

import pandas as pd

# Create a DataFrame with example comments and their labels
examples_data = {
    'Comment ID': ['comment id 1', 'comment id 2', 'comment id 3', 'comment id 4', 'comment id 5'],
    'GT': ['Toxic', 'Non-Toxic', 'Toxic', 'Non-Toxic', 'Toxic'],  # Ground Truth labels
    'Generative': ['Non-Toxic', 'Toxic', 'Non-Toxic', 'Non-Toxic', 'Toxic'],  # Predicted labels by Generative model
    'Discriminative': ['Non-Toxic', 'Toxic', 'Non-Toxic', 'Toxic', 'Toxic']  # Predicted labels by Discriminative model
}

# Create the DataFrame
examples_df = pd.DataFrame(examples_data)

# Print the DataFrame
print("Table 3: Comparing two Models with diverse examples and explaining their output")
print(examples_df)